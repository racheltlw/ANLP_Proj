{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072ce4e9-b7b5-471d-a506-724bedaa8510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rachel\n",
      "[nltk_data]     Tan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc29140-be50-426e-9de8-3771df4f515f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5c2726-cdeb-45fe-87ba-6924bba52ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab9a7b6-439c-45f7-aade-9d53f4484a15",
   "metadata": {},
   "source": [
    "## Trying out just the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bea0346-5367-43f7-9506-c679557071a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rachel Tan\\anaconda3\\envs\\GENVE\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:660: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74b84ee-175d-406d-a6d6-0d3bad195a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\Rachel Tan\\anaconda3\\envs\\GENVE\\lib\\site-packages\\transformers\\generation_utils.py:2138: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Home Team Science and Technology Agency is developing a mobile phone app to help people find their way around the world. The app, which will be available to download as early as October, will be able to track your journey using GPS and GPS-enabled devices.\n",
      "\n",
      "The app will allow users to track their journey using a smartphone or tablet, and will allow them to locate their destination in real-time. The device will be connected to an iPhone or iPad, which can be used to access the app. The Home Team will also be developing a companion app that users can use to track the route of their journey.\n",
      ", which is available to purchase from the Apple App Store. The App Store will also offer a free version of the Home Team app for iPhone and iPad. The new app will be launched in early October. The home team will also continue to develop the app, and can provide feedback on the app in the coming months.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\" #without any fine tuning \n",
    "prompt = \"The Home Team Science and Technology Agency is\"\n",
    "output_length = 200\n",
    "\n",
    "\n",
    "def generate_text(prefix, prompt, output_length=300):\n",
    "    inputs = tokenizer.encode( prefix + prompt,\n",
    "                          return_tensors='pt',\n",
    "                          max_length=512,\n",
    "                          truncation=True)\n",
    "    generated_ids = model.generate(inputs, \n",
    "                             max_length=output_length, \n",
    "                            #  diversity_penalty = 1.2,\n",
    "                             temperature = 1.0,\n",
    "                             do_sample = True,\n",
    "                             no_repeat_ngram_size = 3,\n",
    "                             num_beams=2,\n",
    "                             min_length=20 )\n",
    "    generated_text = tokenizer.decode(generated_ids[0])\n",
    "    print(generated_text)\n",
    "\n",
    "generate_text(prefix, prompt, output_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d4120d-165d-412a-807a-76bbaf44e27a",
   "metadata": {},
   "source": [
    "## Trying with fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1db5b561-2c58-409e-a789-1dc2bc39acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('qa_articles.csv')[['body_basic']] #only 50 for now as a trial, will increase later on \n",
    "dataset.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51824f1c-6f11-4d90-b44a-407f31a4ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = list(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8399bf14-49f1-4467-bff6-a9d61c7fceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(data): \n",
    "    tokenizer.add_special_tokens({'bos_token': '<|startoftext|>', 'eos_token':'<|endoftext|>', 'pad_token': '<|pad|>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    input_text = tokenizer(data,\n",
    "                          max_length=512,\n",
    "                          truncation=True,\n",
    "                          padding = 'max_length')\n",
    "    input_text[\"labels\"] = input_text[\"input_ids\"].copy()\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75a55ec-0ea4-41b2-8dc3-86753e573490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_token = [tokenize_texts(data) for data in dataset_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cec71b3-2a1a-4a02-9969-f841e872146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "152159e9-d1ff-4160-821c-aa1ecb5e1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=data_token, eval_dataset=data_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a282f99d-6f0a-4c65-98db-55ff89e5ad9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 57\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 00:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24, training_loss=15.764266967773438, metrics={'train_runtime': 24.3663, 'train_samples_per_second': 7.018, 'train_steps_per_second': 0.985, 'total_flos': 44680937472000.0, 'train_loss': 15.764266967773438, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "442318d9-9913-4841-8568-bfcda64ba5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./testgpt2\n",
      "Configuration saved in ./testgpt2\\config.json\n",
      "Model weights saved in ./testgpt2\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./testgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff92cddb-ce11-40c2-9887-d177e745f314",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rachel Tan\\anaconda3\\envs\\GENVE\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:660: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "loading configuration file ./testgpt2\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50259\n",
      "}\n",
      "\n",
      "loading weights file ./testgpt2\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./testgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_trained = AutoModelWithLMHead.from_pretrained(\"./testgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea83e668-854b-44ba-aac7-dfeb89016126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_2(prefix, prompt, output_length=300):\n",
    "    tokenizer.add_special_tokens({'bos_token': '<|startoftext|>', 'eos_token':'<|endoftext|>', 'pad_token': '<|pad|>'})\n",
    "    inputs = tokenizer.encode( prefix + prompt,\n",
    "                          return_tensors='pt',\n",
    "                          max_length=200,\n",
    "                          truncation=True)\n",
    "    generated_ids = model_trained.generate(inputs, \n",
    "                             max_length=output_length, \n",
    "                            #  diversity_penalty = 1.2,\n",
    "                             temperature = 1.0,\n",
    "                             do_sample =True ,\n",
    "                             no_repeat_ngram_size =2,\n",
    "                             num_beams=2,\n",
    "                             min_length=100, \n",
    "                            pad_token_id= 50258, \n",
    "                            bos_token_id = 50256,\n",
    "                            eos_token_id = 50257)\n",
    "    generated_text = tokenizer.decode(generated_ids[0])\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4275455a-e389-48e3-b4d6-0f744682663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning <|startoftext|> to the bos_token key of the tokenizer\n",
      "Assigning <|endoftext|> to the eos_token key of the tokenizer\n",
      "Assigning <|pad|> to the pad_token key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|> A question and answering system is one of the most difficult sciences. It consists of four parts: (a<|pad|> a a) The elements of The equations ofThe system consists<|pad|> of. Its contents are: A mathematical formula and the number of elements inThe formula consists Of the equationsof The system contains The numberOf elementsinThe numberof elementsInThe answer to the question, The answerto the questions, is a mathematical solution ofthe question,, isof the answerTo the, the answered to, by, and by.\n",
      "<|startoftext|>\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(88)\n",
    "prefix = \"\" #without any fine tuning \n",
    "prompt = \"<|startoftext|> A question and answering system is\"\n",
    "output_length = 200\n",
    "\n",
    "generate_text_2(prefix, prompt, output_length) #it has been trained but it is crap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d64ded06-ffa7-4c96-bafc-6acf3c831ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|pad|>'})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c06e6e-be92-49ef-a48d-ba1249aabd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#still trying to figure out how to make this work "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
