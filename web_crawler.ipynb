{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9995f0a6-f9f8-4a65-8533-245d892b7ead",
   "metadata": {},
   "source": [
    "## Web Crawler \n",
    "This is a simple web crawler to crawl text from the top results in google search, so that we can pick any topic, easily create a dataset and try to train our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce2f4dfa-bdbc-4a20-82f6-b261c05a8872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from fake_useragent import UserAgent \n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import pandas as pd\n",
    "#pip install pyyaml ua-parser user-agents fake-useragent\n",
    "\n",
    "class crawler(object):\n",
    "    def my_scraper(self, tmp_url_in):\n",
    "        tmp_text = ''\n",
    "        try:\n",
    "            content = requests.get(tmp_url_in)\n",
    "            soup = BeautifulSoup(content.text, 'html.parser')\n",
    "    \n",
    "            tmp_text = soup.findAll('p') \n",
    "    \n",
    "            tmp_text = [word.text for word in tmp_text]\n",
    "            tmp_text = ' '.join(tmp_text)\n",
    "            tmp_text = re.sub('\\W+', ' ', re.sub('xa0', ' ', tmp_text))\n",
    "            #tmp_text = re.sub('\\W+', ' ', tmp_text)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        return tmp_text\n",
    "    \n",
    "    def fetch_urls(self, query, cnt):\n",
    "        #now lets use the following function that returns\n",
    "        #URLs from an arbitrary regex crawl form google\n",
    "        \n",
    "        ua = UserAgent()\n",
    "    \n",
    "        google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(cnt)\n",
    "        response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "        result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})\n",
    "    \n",
    "        links = []\n",
    "        titles = []\n",
    "        descriptions = []\n",
    "        for r in result_div:\n",
    "            # Checks if each element is present, else, raise exception\n",
    "            try:\n",
    "                link = r.find('a', href = True)\n",
    "                title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
    "                description = r.find('div', attrs={'class':'s3v9rd'}).get_text()\n",
    "    \n",
    "                # Check to make sure everything is present before appending\n",
    "                if link != '' and title != '' and description != '': \n",
    "                    links.append(link['href'])\n",
    "                    titles.append(title)\n",
    "                    descriptions.append(description)\n",
    "            # Next loop if one element is not present\n",
    "            except:\n",
    "                print('something is missing!')\n",
    "                continue  \n",
    "    \n",
    "        to_remove = []\n",
    "        clean_links = []\n",
    "        for i, l in enumerate(links):\n",
    "            clean = re.search('\\/url\\?q\\=(.*)\\&sa',l)\n",
    "    \n",
    "            # Anything that doesn't fit the above pattern will be removed\n",
    "            if clean is None:\n",
    "                to_remove.append(i)\n",
    "                continue\n",
    "            clean_links.append(clean.group(1))\n",
    "    \n",
    "        # Remove the corresponding titles & descriptions\n",
    "        for x in to_remove:\n",
    "            del titles[x]\n",
    "            del descriptions[x]\n",
    "            \n",
    "        return clean_links\n",
    "    \n",
    "    def create_dataframe(self, my_query, the_cnt_in): \n",
    "        the_urls_list = self.fetch_urls(my_query, the_cnt_in)\n",
    "        the_data = pd.DataFrame()\n",
    "        for word in the_urls_list:\n",
    "            body_basic = self.my_scraper(word)\n",
    "            the_data = the_data.append({'body_basic': body_basic, 'url': word}, ignore_index = True)\n",
    "        return the_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6245e639-9605-499c-884c-8d738bc0ec7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for  Question and Answering System\n",
      "something is missing!\n",
      "something is missing!\n",
      "something is missing!\n",
      "something is missing!\n",
      "something is missing!\n"
     ]
    }
   ],
   "source": [
    "#Crawl the 10 URLS from the output of fetch_urls and append to a dataframe the following; \n",
    "#url in one column and cleaned text in the other \n",
    "the_query = ['Home Team Science and Technology Agency', 'Question and Answering System']\n",
    "num_docs =  100\n",
    "dataframe_list = []\n",
    "for item in the_query: \n",
    "    dataframe = pd.DataFrame()\n",
    "    print('searching for ', item)\n",
    "    my_func = crawler()\n",
    "    dataframe = dataframe.append(my_func.create_dataframe(item, num_docs))\n",
    "    dataframe= dataframe[dataframe.body_basic != '']\n",
    "    dataframe['length'] = dataframe.body_basic.apply(lambda x: len(x.split(' ')))\n",
    "    dataframe = dataframe[dataframe.length > 50] #we only want articles where clean text is more than 50 words\n",
    "    dataframe_list.append(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f74197c5-3e02-4c14-8d4d-a48947ed8d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_basic</th>\n",
       "      <th>url</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Home Team Science and Technology Agency ab...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Home_Team_Scienc...</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 Stars Avenue 12 01 Singapore 138507 1 Stars ...</td>\n",
       "      <td>https://www.sgdi.gov.sg/ministries/mha/statuto...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Copyright Ministry of Health Singapore All Ri...</td>\n",
       "      <td>https://www.healthhub.sg/directory/16/67277/ho...</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Home Team keeps Singapore safe and secure ...</td>\n",
       "      <td>https://www.mha.gov.sg/who-we-are</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>463 Restricted Client This resource is not ava...</td>\n",
       "      <td>https://sso.agc.gov.sg/Act/HTSTAA2019</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          body_basic  \\\n",
       "1  The Home Team Science and Technology Agency ab...   \n",
       "3  1 Stars Avenue 12 01 Singapore 138507 1 Stars ...   \n",
       "4   Copyright Ministry of Health Singapore All Ri...   \n",
       "5  The Home Team keeps Singapore safe and secure ...   \n",
       "6  463 Restricted Client This resource is not ava...   \n",
       "\n",
       "                                                 url  length  \n",
       "1  https://en.wikipedia.org/wiki/Home_Team_Scienc...     181  \n",
       "3  https://www.sgdi.gov.sg/ministries/mha/statuto...      59  \n",
       "4  https://www.healthhub.sg/directory/16/67277/ho...     166  \n",
       "5                  https://www.mha.gov.sg/who-we-are     756  \n",
       "6              https://sso.agc.gov.sg/Act/HTSTAA2019      72  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htx = dataframe_list[0]\n",
    "htx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61d95ae7-d2fb-4491-869b-9441416adea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_basic</th>\n",
       "      <th>url</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question answering QA is a computer science di...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Question_answering</td>\n",
       "      <td>1596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recently I came across this library that enabl...</td>\n",
       "      <td>https://www.section.io/engineering-education/q...</td>\n",
       "      <td>961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please contact us via our support center for ...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sign in Alvira Swalin May 23 2018 9 min read F...</td>\n",
       "      <td>https://towardsdatascience.com/building-a-ques...</td>\n",
       "      <td>1679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>but your activity and behavior on this site m...</td>\n",
       "      <td>https://iopscience.iop.org/article/10.1088/175...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          body_basic  \\\n",
       "0  Question answering QA is a computer science di...   \n",
       "1  Recently I came across this library that enabl...   \n",
       "2   Please contact us via our support center for ...   \n",
       "3  Sign in Alvira Swalin May 23 2018 9 min read F...   \n",
       "4   but your activity and behavior on this site m...   \n",
       "\n",
       "                                                 url  length  \n",
       "0   https://en.wikipedia.org/wiki/Question_answering    1596  \n",
       "1  https://www.section.io/engineering-education/q...     961  \n",
       "2  https://www.sciencedirect.com/science/article/...      93  \n",
       "3  https://towardsdatascience.com/building-a-ques...    1679  \n",
       "4  https://iopscience.iop.org/article/10.1088/175...      72  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = dataframe_list[1]\n",
    "qa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82fd752b-7e59-4b30-8533-98fd911d6e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "htx[['body_basic']].to_csv('htx_articles.csv', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dec084e-420a-49c8-8204-5918b3323ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa[['body_basic']].to_csv('qa_articles.csv', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6bbdd-aa26-49c4-99f6-03f163ab652a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
